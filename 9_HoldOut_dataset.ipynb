{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm SVD on 3 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Mean    Std     \n",
      "RMSE (testset)    0.8849  0.8834  0.8820  0.8834  0.0012  \n",
      "MAE (testset)     0.6989  0.6983  0.6958  0.6977  0.0013  \n",
      "Fit time          127.77  127.55  126.95  127.42  0.34    \n",
      "Test time         1.48    1.41    1.30    1.40    0.08    \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from surprise import SVD\n",
    "from surprise import SVDpp\n",
    "from surprise import NMF\n",
    "from surprise import KNNBasic\n",
    "from surprise import KNNWithZScore\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "from surprise.model_selection import GridSearchCV\n",
    "\n",
    "my_seed = 0\n",
    "random.seed(my_seed)\n",
    "np.random.seed(my_seed)\n",
    "\n",
    "# Train data set with Normalized Purchase\n",
    "User_Prod_NP = pd.read_csv('./Files_Folder/User_Prod_NormPurc.csv')\n",
    "\n",
    "# Statistics of each Product_ID in train data sets\n",
    "train_Prod_sta = pd.read_csv('./Files_Folder/train_User_Prod_stat.csv') \n",
    "\n",
    "# Statistics of new Product_IDs; those are Prodcut_IDs in test data set but not in train data set\n",
    "New_Prod_LUT = pd.read_csv('./Files_Folder/New_Product_Details.csv')\n",
    "\n",
    "# Hold Data With Purchase\n",
    "hold_data = pd.read_csv('./Files_Folder/hold_data.csv')\n",
    "\n",
    "# Hold Out Data Set\n",
    "hold_data_sub = pd.read_csv('./Files_Folder/hold_data_sub.csv')\n",
    "\n",
    "# Train Data Set\n",
    "train_data = pd.read_csv('./Files_Folder/train_data.csv')\n",
    "\n",
    "algo_SVD = SVD(n_epochs=130, lr_all = 0.005, reg_all = 0.1)\n",
    "\n",
    "# A reader is still needed but only the rating_scale param is requiered.\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "data = Dataset.load_from_df(User_Prod_NP[['User_ID', 'Product_ID', 'Normalized_Purchase']], reader)\n",
    "\n",
    "# We can now use this dataset as we please, e.g. calling cross_validate\n",
    "cross_validate(algo_SVD, data, cv=3, verbose = True)\n",
    "\n",
    "new_prod = list(New_Prod_LUT.Product_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking out the hold data set and computing the rmse\n",
    "hold_user_list = []\n",
    "hold_prod_list = []\n",
    "price = []\n",
    "for i in range(hold_data_sub.shape[0]):\n",
    "    if hold_data_sub.Product_ID[i] in new_prod:\n",
    "        p = New_Prod_LUT[New_Prod_LUT.Product_ID == hold_data_sub.Product_ID[i]]['mean'].iloc[0]      \n",
    "    else:\n",
    "        p1 = algo_SVD.predict(hold_data_sub.User_ID[i],hold_data_sub.Product_ID[i]).est\n",
    "        rmin = train_Prod_sta.loc[3, hold_data_sub.Product_ID[i]]\n",
    "        rmax = train_Prod_sta.loc[7, hold_data_sub.Product_ID[i]]\n",
    "        dmin = 1\n",
    "        dmax = 5\n",
    "        p = (p1 - dmin)*(rmax - rmin)/(dmax - dmin) + rmin\n",
    "    price.append(p)\n",
    "    hold_user_list.append(hold_data_sub.User_ID[i])\n",
    "    hold_prod_list.append(hold_data_sub.Product_ID[i])\n",
    "final_tally = {'User_ID':hold_user_list, 'Product_ID':hold_prod_list, 'Purchase': price}\n",
    "final_tally_df = pd.DataFrame(final_tally)\n",
    "final_tally_df.to_csv('./Files_Folder/Predicted_hold_SVD_mean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2414.796149517017\n"
     ]
    }
   ],
   "source": [
    "final_tally_df['Actual_Purchase'] = hold_data['Purchase']\n",
    "final_tally_df['Error'] = final_tally_df['Purchase'] - final_tally_df['Actual_Purchase']\n",
    "final_tally_df['Error_sq'] = final_tally_df['Error']*final_tally_df['Error']\n",
    "rmse_hold_data_pred = np.sqrt(final_tally_df.Error_sq.sum()/final_tally_df.shape[0])\n",
    "print (rmse_hold_data_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
